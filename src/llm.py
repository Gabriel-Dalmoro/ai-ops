from __future__ import annotations
import os
from typing import Optional
import google.generativeai as genai
from loguru import logger

# --- simple token helpers (rough but safe for free tiers) ---
def estimate_tokens(text: str) -> int:
    return max(1, len(text) // 4)

def truncate_by_tokens(text: str, max_tokens: int) -> str:
    if not text:
        return ""
    if estimate_tokens(text) <= max_tokens:
        return text
    char_budget = max_tokens * 4
    # Ensure we don't cut in the middle of a word for the truncation message
    return text[:char_budget].rsplit(' ', 1)[0] + "\nâ€¦[truncated]"

# --- minimal LLM adapter ---
class LLM:
    """
    Minimal, pluggable LLM adapter with guardrails.

    Reads configuration from your .env file:
      MODEL_BACKEND=gemini
      GEMINI_API_KEY=...
      GEMINI_MODEL_NAME=gemini-1.5-flash-latest
    """

    def __init__(self, backend: Optional[str] = None):
        self.backend = (backend or os.getenv("MODEL_BACKEND", "stub")).lower()
        # Guardrail defaults
        self.max_prompt_tokens = int(os.getenv("MAX_PROMPT_TOKENS", "2000"))
        self.max_output_tokens = int(os.getenv("MAX_OUTPUT_TOKENS", "800"))
        self.temperature = float(os.getenv("LLM_TEMPERATURE", "0.25"))

        if self.backend == "gemini":
            self.api_key = os.getenv("GEMINI_API_KEY")
            if not self.api_key:
                raise ValueError("Missing GEMINI_API_KEY in environment for Gemini backend.")
            
            # --- NEW: Configurable model name ---
            self.model_name = os.getenv("GEMINI_MODEL_NAME", "gemini-1.5-flash-latest")
            
            genai.configure(api_key=self.api_key)
            logger.info(f"Gemini backend configured with model: {self.model_name}")

    def generate(self, prompt: str) -> str:
        safe_prompt = truncate_by_tokens(prompt, self.max_prompt_tokens)

        if self.backend == "stub":
            logger.info("Using 'stub' backend for LLM call.")
            return (
                "[STUB TAILOR OUTPUT]\n\n"
                + "This is a placeholder generated by the stub model."
                + "\n\n(Set MODEL_BACKEND=gemini and provide a GEMINI_API_KEY to use a real model.)"
            )

        if self.backend == "gemini":
            logger.info(f"Calling Gemini API with model '{self.model_name}'...")
            
            # --- UPDATED: Use the configurable model name ---
            model = genai.GenerativeModel(self.model_name)
            
            generation_config = genai.types.GenerationConfig(
                max_output_tokens=self.max_output_tokens,
                temperature=self.temperature,
            )
            
            try:
                response = model.generate_content(safe_prompt, generation_config=generation_config)
                logger.success("Successfully received response from Gemini.")
                return response.text
            except Exception as e:
                logger.error(f"Error calling Gemini API: {e}")
                return f"[ERROR] Failed to generate text from Gemini: {e}"

        raise ValueError(f"Unknown MODEL_BACKEND: {self.backend}")